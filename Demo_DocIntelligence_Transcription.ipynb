{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "BiayQrAV-iJP",
   "metadata": {
    "id": "BiayQrAV-iJP"
   },
   "source": [
    "# Best Practices\n",
    "1. Save a local copy of this notebook to be included in your archive.\n",
    "2. Save the cleaned text and also the raw OCR text and corresponding note files.\n",
    "3. Remove the key and endpoint information from any saved copy that will be publicly available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-A9aLEengvnz",
   "metadata": {
    "id": "-A9aLEengvnz"
   },
   "source": [
    "## I. Using Microsoft Azure's Document Intelligence to Transcribe Documents\n",
    "\n",
    "### Run this to import the required code libraries from Microsoft Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jap7rW4JxBNy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jap7rW4JxBNy",
    "outputId": "4b62eb25-ae55-412b-df49-278c320ec3ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-documentintelligence\n",
      "  Downloading azure_ai_documentintelligence-1.0.0b4-py3-none-any.whl.metadata (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting isodate>=0.6.1 (from azure-ai-documentintelligence)\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting azure-core>=1.30.0 (from azure-ai-documentintelligence)\n",
      "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from azure-ai-documentintelligence) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-ai-documentintelligence) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from azure-core>=1.30.0->azure-ai-documentintelligence) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.21.0->azure-core>=1.30.0->azure-ai-documentintelligence) (2024.8.30)\n",
      "Downloading azure_ai_documentintelligence-1.0.0b4-py3-none-any.whl (99 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.9/198.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Installing collected packages: isodate, azure-core, azure-ai-documentintelligence\n",
      "Successfully installed azure-ai-documentintelligence-1.0.0b4 azure-core-1.32.0 isodate-0.7.2\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ai-documentintelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iD_YZYp6-vd7",
   "metadata": {
    "id": "iD_YZYp6-vd7"
   },
   "source": [
    "### Create folders for the data (raw OCR) and output (cleaned OCR text files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plhRSqNb9nog",
   "metadata": {
    "id": "plhRSqNb9nog"
   },
   "outputs": [],
   "source": [
    "!mkdir data\n",
    "!mkdir notes\n",
    "!mkdir cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74FIfGLy_C8Z",
   "metadata": {
    "id": "74FIfGLy_C8Z"
   },
   "source": [
    "### Upload PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ix9xDO4mYZvX",
   "metadata": {
    "id": "ix9xDO4mYZvX"
   },
   "source": [
    "#### OPTION 1: Upload from Local Drive\n",
    "Use this to upload the unprocessed PDFs or any text files you wish to clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O6m6V_eC-rjV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "id": "O6m6V_eC-rjV",
    "outputId": "c27d8e13-9876-4c8d-c357-95655509d276"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-667988ad-1e8c-4d04-a4fe-1b4778d54d77\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-667988ad-1e8c-4d04-a4fe-1b4778d54d77\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving CH_T-02_Aklavik.pdf to CH_T-02_Aklavik.pdf\n",
      "Saving CH_T-03_Aklavik.pdf to CH_T-03_Aklavik.pdf\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8GDmTfjai4K",
   "metadata": {
    "id": "a8GDmTfjai4K"
   },
   "source": [
    "Define text parsing and file saving functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XIOQ1HMPabrE",
   "metadata": {
    "id": "XIOQ1HMPabrE"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# USED BY DOCUMENT INTELLIGENCE\n",
    "\n",
    "# Get words\n",
    "def get_words(page, line):\n",
    "    result = []\n",
    "    for word in page.words:\n",
    "        if _in_span(word, line.spans):\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "# To learn the detailed concept of \"span\" in the following codes, visit: https://aka.ms/spans\n",
    "def _in_span(word, spans):\n",
    "    for span in spans:\n",
    "        if word.span.offset >= span.offset and (word.span.offset + word.span.length) <= (span.offset + span.length):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# USED TO INTERACT WITH FILES\n",
    "\n",
    "sourcepath = './'\n",
    "outputdir = './data/'\n",
    "\n",
    "# Save text (txt) to a new file (filename)\n",
    "def save_to_file(filename,txt):\n",
    "    wfile = open(filename,\"w\")\n",
    "    wfile.write(txt)\n",
    "    wfile.close()\n",
    "\n",
    "# Read/transcribe a pdf and save it to the data directory\n",
    "def read_pdf_into_ocr(fname,sourcedir,outputdir):\n",
    "\n",
    "    try:\n",
    "        filename = sourcedir+fname\n",
    "        result_text = analyze_read(sourcedir,fname)\n",
    "        newfile = fname.split('.')\n",
    "        newfileloc = outputdir+newfile[0]\n",
    "\n",
    "        save_to_file(newfileloc+'_ocr.txt',result_text)\n",
    "\n",
    "    except:\n",
    "        print('Error while reading pdf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9JEkmI-whVev",
   "metadata": {
    "id": "9JEkmI-whVev"
   },
   "source": [
    "### REQUIRED: Enter your Endpoint and Key values\n",
    "\n",
    "### **Without these credentials, the Document Intelligence functions won't work**\n",
    "\n",
    "*   Replace \"YOUR_FORM_RECOGNIZER_ENDPOINT\" with Endpoint value provided you by Microsoft Azure\n",
    "*   Replace \"YOUR_FORM_RECOGNIZER_KEY\" with the Key value provided by Microsoft Azure\n",
    "\n",
    "### **These key credentials are from your paid account and should not be shared publicly.**\n",
    "\n",
    "Remember to remove the key from your code when you're done, and never post it publicly. For production, use secure methods to store and access your credentials.\n",
    "\n",
    "For a walkthrough on setting up Document Intelligence on Azure and finding your Endpoint and Key, visit:\n",
    "* https://github.com/DiSA-Projects/Doc-Intel-Transcription/blob/main/SetupDocIntel.md\n",
    "\n",
    "You can read more about how to use and extend Document Intelligence here:\n",
    "* https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-security?tabs=command-line%2Ccsharp#environment-variables-and-application-configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "so_gZ0PlhLzE",
   "metadata": {
    "id": "so_gZ0PlhLzE"
   },
   "outputs": [],
   "source": [
    "endpoint = \"YOUR_FORM_RECOGNIZER_ENDPOINT\"  # Replace with Endpoint from Azure Document Intelligence\n",
    "key = \"YOUR_FORM_RECOGNIZER_KEY\"            # Replace with Key from Azure Document Intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MCFTsBE5jeew",
   "metadata": {
    "id": "MCFTsBE5jeew"
   },
   "source": [
    "### Defines the Analyze_Read function used by Document Intelligence to transcribe texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uezawuO7lUsA",
   "metadata": {
    "id": "uezawuO7lUsA"
   },
   "outputs": [],
   "source": [
    "# Modified version of Analyze_Read function provided as a template by Microsoft Azure Document Intelligence\n",
    "# * Instead of printing output to the terminal, the analytical notes are saved in a separate text file in notes.\n",
    "\n",
    "def analyze_read(sourcepath,fname):\n",
    "    filetext = ''\n",
    "    filenotes = ''\n",
    "    filename = sourcepath+fname\n",
    "\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "    from azure.ai.documentintelligence.models import DocumentAnalysisFeature, AnalyzeResult, AnalyzeDocumentRequest\n",
    "\n",
    "    # For how to obtain the endpoint and key, please see PREREQUISITES above.\n",
    "    #endpoint = os.environ[\"DOCUMENTINTELLIGENCE_ENDPOINT\"]\n",
    "    #key = os.environ[\"DOCUMENTINTELLIGENCE_API_KEY\"]\n",
    "\n",
    "    document_intelligence_client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "\n",
    "    # Analyze a document at a URL:\n",
    "    # formUrl = \"https://raw.githubusercontent.com/Azure-Samples/cognitive-services-REST-api-samples/master/curl/form-recognizer/rest-api/read.png\"\n",
    "    # Replace with your actual formUrl:\n",
    "    # If you use the URL of a public website, to find more URLs, please visit: https://aka.ms/more-URLs\n",
    "    # If you analyze a document in Blob Storage, you need to generate Public SAS URL, please visit: https://aka.ms/create-sas-tokens\n",
    "    # poller = document_intelligence_client.begin_analyze_document(\n",
    "    #    \"prebuilt-read\",\n",
    "    #    AnalyzeDocumentRequest(url_source=formUrl),\n",
    "    #    features=[DocumentAnalysisFeature.LANGUAGES]\n",
    "    # )\n",
    "\n",
    "    # # If analyzing a local document, remove the comment markers (#) at the beginning of these 11 lines.\n",
    "    # Delete or comment out the part of \"Analyze a document at a URL\" above.\n",
    "    # Replace <path to your sample file>  with your actual file path.\n",
    "    path_to_sample_document = filename\n",
    "    print(f'Filename {filename}')\n",
    "    with open(path_to_sample_document, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-read\",\n",
    "            analyze_request=f,\n",
    "            features=[DocumentAnalysisFeature.LANGUAGES],\n",
    "            content_type=\"application/octet-stream\",\n",
    "        )\n",
    "    result: AnalyzeResult = poller.result()\n",
    "\n",
    "    # [START analyze_read]\n",
    "    # Detect languages.\n",
    "    print(\"----Languages detected in the document----\")\n",
    "    if result.languages is not None:\n",
    "        for language in result.languages:\n",
    "            filenotes = filenotes + '/n'+'Language code: '+language.locale+' with confidence '+str(language.confidence)\n",
    "            #print(f\"Language code: '{language.locale}' with confidence {language.confidence}\")\n",
    "\n",
    "    # To learn the detailed concept of \"bounding polygon\" in the following content, visit: https://aka.ms/bounding-region\n",
    "    # Analyze pages.\n",
    "    for page in result.pages:\n",
    "        #print(f\"----Analyzing document from page #{page.page_number}----\")\n",
    "        filenotes = filenotes+'/n'+'----Analyzing document from page #'+str(page.page_number)+'----'\n",
    "        filenotes = filenotes+'/n'+'Page has width: '+str(page.width)+' and height: '+str(page.height)+', measured with unit: '+page.unit\n",
    "        #print(f\"Page has width: {page.width} and height: {page.height}, measured with unit: {page.unit}\")\n",
    "\n",
    "        # Analyze lines.\n",
    "        if page.lines:\n",
    "            for line_idx, line in enumerate(page.lines):\n",
    "                words = get_words(page, line)\n",
    "                #print(\n",
    "                #    f\"...Line # {line_idx} has {len(words)} words and text '{line.content}' within bounding polygon '{line.polygon}'\"\n",
    "                #)\n",
    "                filenotes = filenotes+'\\n'+'...Line # '+str(line_idx)+' has '+str(len(words))+ ' words and text '+line.content+' within bounding polygon '+str(line.polygon)\n",
    "\n",
    "                # Analyze words.\n",
    "                for word in words:\n",
    "                    #print(f\"......Word '{word.content}' has a confidence of {word.confidence}\")\n",
    "                    filenotes = filenotes+'\\n'+'......Word '+ word.content+' has a confidence of '+str(word.confidence)\n",
    "\n",
    "    # Analyze paragraphs.\n",
    "    if result.paragraphs:\n",
    "        print(f\"----Detected #{len(result.paragraphs)} paragraphs in the document: {fname}----\")\n",
    "        for paragraph in result.paragraphs:\n",
    "            #print(f\"Found paragraph within {paragraph.bounding_regions} bounding region\")\n",
    "            #print(f\"...with content: '{paragraph.content}'\")\n",
    "            filetext = filetext + '\\n'+paragraph.content\n",
    "\n",
    "    newfile = fname.split('.')\n",
    "    newfileloc = './notes/'+newfile[0]+'_notes.txt'\n",
    "    save_to_file(newfileloc,filenotes)\n",
    "    print(\"----------------------------------------\")\n",
    "    filetext = filetext + '\\n----------------------------------------'\n",
    "    return filetext\n",
    "# [END analyze_read]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qi8owpJ8ob_Y",
   "metadata": {
    "id": "Qi8owpJ8ob_Y"
   },
   "source": [
    "### Convert PDFs to text files\n",
    "Using Microsoft Azure's Document Intelligence (AI-aided transcription) to read, identify words, and produce a transcribed text version of the PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fDQlT7zXfnbW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fDQlT7zXfnbW",
    "outputId": "369215f2-7b46-48df-8f6b-f525a3d7e7b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENDPOINT https://westus2.api.cognitive.microsoft.com/\n",
      "Filename ./CH_T-02_Aklavik.pdf\n",
      "----Languages detected in the document----\n",
      "----Detected #826 paragraphs in the document: CH_T-02_Aklavik.pdf----\n",
      "----------------------------------------\n",
      "ENDPOINT https://westus2.api.cognitive.microsoft.com/\n",
      "Filename ./CH_T-03_Aklavik.pdf\n",
      "----Languages detected in the document----\n",
      "----Detected #312 paragraphs in the document: CH_T-03_Aklavik.pdf----\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "files = [f for f in os.listdir(sourcepath) if os.path.isfile(os.path.join(sourcepath, f))]\n",
    "for f in files:\n",
    "  read_pdf_into_ocr(f,sourcepath,outputdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FNl8ElqqjsTf",
   "metadata": {
    "id": "FNl8ElqqjsTf"
   },
   "source": [
    "## II. Processing and Cleaning Transcripts (after Document Intelligence is done)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67794b-f02a-48ff-a988-6443c56eacfb",
   "metadata": {
    "id": "8d67794b-f02a-48ff-a988-6443c56eacfb"
   },
   "source": [
    "## Text File Cleaning Template\n",
    "\n",
    "This Python code removes unwanted line numbers, paragraph markers, details about the digitization company, and page numbers from the raw text files generated by AI-assisted transcription (the output from using Microsoft Azure's Document Intelligence). The output should be human-readable text suitable for textual analysis. Additional functions can be written to clean up other details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7DUgipb1v_i6",
   "metadata": {
    "id": "7DUgipb1v_i6"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "334f57a5-e127-43be-b4ad-857d37012543",
   "metadata": {
    "id": "334f57a5-e127-43be-b4ad-857d37012543"
   },
   "source": [
    "#### Import Python libraries for Regular Expressions and Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a86dc8-35bc-4996-a693-ee6aa9452328",
   "metadata": {
    "id": "22a86dc8-35bc-4996-a693-ee6aa9452328"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f310e3-ac53-489b-af67-dbbfdb807ae6",
   "metadata": {
    "id": "e0f310e3-ac53-489b-af67-dbbfdb807ae6"
   },
   "source": [
    "#### Function to remove paragraph and section markers created by AI transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0325b0ad-e067-4d3c-a954-5bb019713e93",
   "metadata": {
    "id": "0325b0ad-e067-4d3c-a954-5bb019713e93"
   },
   "outputs": [],
   "source": [
    "def remove_paragraph_markers(text):\n",
    "    #Remove \"Paragraph:\" and \"=======\" using regular expressions\n",
    "    text = re.sub(r'(Paragraph:)','',text)\n",
    "    text = re.sub(r'===PARAGRAPH===','',text)\n",
    "    text = re.sub(r'={10}','',text)\n",
    "    text = re.sub(r'\\n{2}','',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3245e607-70e0-45ef-a1f6-21bcc2ad7155",
   "metadata": {
    "id": "3245e607-70e0-45ef-a1f6-21bcc2ad7155"
   },
   "source": [
    "#### Remove any language related to the digital transcription company that digitized/scanned the text/document\n",
    "\n",
    "You can customize this part to remove specific strings from appearing in the final version (eg. names and addresses of the transcription service that scanned these documents, rather than the content itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff2e453-6096-4a6d-ab57-145489949085",
   "metadata": {
    "id": "dff2e453-6096-4a6d-ab57-145489949085"
   },
   "outputs": [],
   "source": [
    "def remove_transcription_comp(text):\n",
    "    # Example of removing references to a transcription company's name, phone numbers, and website since they are not relevant to our research\n",
    "    text = re.sub(r'FALLGUY REPORTING LTD.','',text)\n",
    "    text = re.sub(r'(Ph: 604-555-4444 Fax: 604-333-8888)','',text)\n",
    "    text = re.sub(r'(www.fallguyservice.com)','',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6136d2d3-6069-4db0-8f41-073cf2c79384",
   "metadata": {
    "id": "6136d2d3-6069-4db0-8f41-073cf2c79384"
   },
   "source": [
    "#### Remove any non-relevant artifacts and strings embedded in the text.\n",
    "\n",
    "For legal documents and court transcripts, there will often be line numbers on the left margin. Sometimes Document Intelligence will misinterpret hole punch marks as characters -- these too can be removed.\n",
    "\n",
    "**Remove Line Numbers**\n",
    "* The current code removes just line numbers that appear at the start of a line.\n",
    "* This also removes page numbers if they appear alone on a line.\n",
    "* If removing a line number produces a blank line, the blank line is removed (this can be changed if needed).\n",
    "* Volume number and dates are left in place (no changes made to them).\n",
    "\n",
    "**Remove Hole Punch Marks**\n",
    "* The holes left by a three-hole punch are often misinterpreted as 0, O, or : -- this function removes any solo appearance of these characters (alone on a line).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35a8550-8b82-49d9-8b91-ea05a5dddbeb",
   "metadata": {
    "id": "e35a8550-8b82-49d9-8b91-ea05a5dddbeb"
   },
   "outputs": [],
   "source": [
    "def is_all_numbers(line):\n",
    "    numlist = line.split(' ')\n",
    "    for n in numlist:\n",
    "        if re.match('^\\d{1,2}',n) == None:\n",
    "            if not n in ['!','i','O',':','.','C']:\n",
    "                return False\n",
    "            #n = re.sub('^\\d{1,2}','',n)\n",
    "    return True\n",
    "\n",
    "def remove_line_numbers(text):\n",
    "    newtext=''\n",
    "\n",
    "    # Search the file to see if there are any lines that begin with a number\n",
    "    matches = re.findall(r'\\n\\d+',text)\n",
    "\n",
    "    # If lines starting with numbers exist, examine each line\n",
    "    if len(matches)>0:\n",
    "        # Split the file into a list of lines\n",
    "        lines = text.split('\\n')\n",
    "        for line in lines:\n",
    "            # Check to see if line begins with a 1 or 2 digit number\n",
    "            if not re.match('\\d{1,2}',line) == None:\n",
    "                # Remove numbers from the start of the line\n",
    "                line = re.sub('\\d{1,2}','',line)\n",
    "\n",
    "            # Remove any leading spaces leftover from removing the line number\n",
    "            if line.startswith(' '):\n",
    "                line = line[1:]\n",
    "            if len(line)>0:\n",
    "                # If the line is not blank, add back to the file\n",
    "                newtext=newtext+'\\n'+line\n",
    "    else:\n",
    "        # If there are no lines which begin with numbers, just return the original text\n",
    "        return text\n",
    "\n",
    "    # Return the updated version of the file\n",
    "    return newtext\n",
    "\n",
    "# Remove misinterpreted hole punch marks. Can be modified to catch additional ways a hole punch is misread\n",
    "def remove_hole_punch_marks(text):\n",
    "    newtext = ''\n",
    "    lines = text.split('\\n')\n",
    "    for line in lines:\n",
    "        if len(line) == 1 and line[0] in ['O','0',':']:\n",
    "            newtext = newtext+'\\n'\n",
    "        else:\n",
    "            newtext = newtext+'\\n'+line\n",
    "\n",
    "    return newtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a82764-f921-4f2c-bc5c-2e04e03fcdde",
   "metadata": {
    "id": "30a82764-f921-4f2c-bc5c-2e04e03fcdde"
   },
   "source": [
    "#### Call the above functions to clean the text in a given file\n",
    "Comment out any function calls that are not needed (just put a # in front of a line)\n",
    "\n",
    "**NOTE:** It is strongly recommended that you create a new function for each step of data cleaning you are attempting, rather than put all text cleaning substitutions in the same function. This will make it easier to debug and gives you more control.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d495b340-08ad-4cac-824a-5807f4d86808",
   "metadata": {
    "id": "d495b340-08ad-4cac-824a-5807f4d86808"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = remove_transcription_comp(text)\n",
    "    text = remove_paragraph_markers(text)\n",
    "    text = remove_hole_punch_marks(text)\n",
    "    text = remove_line_numbers(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SWj0NoE2lsg_",
   "metadata": {
    "id": "SWj0NoE2lsg_"
   },
   "source": [
    "#### (OPTIONAL) Upload/Import the raw data files here (if you didn't use Document Intelligence above)\n",
    "If you are just testing text cleaning steps or aren't using Document Intelligence for transcription, you can use the snippet below to import raw text files from your local drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BJVkpiS6MF_W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 38
    },
    "id": "BJVkpiS6MF_W",
    "outputId": "0641e672-44c8-4c90-c4a5-631f772b599d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2704a6f8-83f8-4670-9f45-eb7cf0b79957\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-2704a6f8-83f8-4670-9f45-eb7cf0b79957\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "D3OBq_0SnDn2",
   "metadata": {
    "id": "D3OBq_0SnDn2"
   },
   "source": [
    "#### After importing files, wait a moment for the File Explorer on the left to refresh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417dc7e5-0c98-47d3-8157-2265ef9e1382",
   "metadata": {
    "id": "417dc7e5-0c98-47d3-8157-2265ef9e1382"
   },
   "source": [
    "#### Read a raw text file and output a clean version\n",
    "\n",
    "- Cleaned text will be saved in the **/cleaned** folder\n",
    "- As the code runs, the name of the data file currently being processed will be printed out, as will the name of the output file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caba3fa-cb02-421e-8857-1928a4d97def",
   "metadata": {
    "id": "0caba3fa-cb02-421e-8857-1928a4d97def"
   },
   "outputs": [],
   "source": [
    "def readDataFile(filename,datadir,outputdir):\n",
    "    import os\n",
    "    try:\n",
    "        with open(datadir+filename,'r') as text:\n",
    "            lines = text.read()\n",
    "            cleaned = clean_text(lines)\n",
    "            fname = filename.split('.')\n",
    "            newfile = fname[0]+'_clean.txt'\n",
    "            wfile = open('.'+outputdir+newfile,\"w\")\n",
    "            wfile.write(cleaned)\n",
    "            wfile.close()\n",
    "            #print(cleaned)\n",
    "            print(filename+' processed. Creating '+newfile)\n",
    "    except:\n",
    "        print('Configuration file read error')\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "O-RnwEV1QRkS",
   "metadata": {
    "id": "O-RnwEV1QRkS"
   },
   "source": [
    "### Process data files\n",
    "Run this snippet to process/clean the data files. Cleaned versions of the text files will appear when you **refresh** the file explorer on the left.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a55b9ba-4928-46d0-9d67-88f438a83cef",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4a55b9ba-4928-46d0-9d67-88f438a83cef",
    "outputId": "6ea789fc-61dc-41c3-c249-3fc6adb35c75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CH_T-03_Aklavik_ocr.txt processed. Creating CH_T-03_Aklavik_ocr_clean.txt\n",
      "CH_T-02_Aklavik_ocr.txt processed. Creating CH_T-02_Aklavik_ocr_clean.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "datadir = './data/'\n",
    "outputdir = '/cleaned/'\n",
    "datafiles = [f for f in os.listdir(datadir) if os.path.isfile(os.path.join(datadir, f))]\n",
    "for f in datafiles:\n",
    "    readDataFile(f,datadir,outputdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nvXdRoL_ipOY",
   "metadata": {
    "id": "nvXdRoL_ipOY"
   },
   "source": [
    "## III: Zip archived raw, annotated, and cleaned files in separate files for easy download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ua7PtsOZD18",
   "metadata": {
    "id": "8ua7PtsOZD18"
   },
   "source": [
    "### OPTIONAL: Create a Zip file containing all processed files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sGn4OU1-tENE",
   "metadata": {
    "id": "sGn4OU1-tENE"
   },
   "source": [
    "Add cleaned files to data-clean.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LYNB26lNW36J",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LYNB26lNW36J",
    "outputId": "19ec0c41-272b-4c51-8fb1-07c5aa542b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/cleaned/ (stored 0%)\n",
      "  adding: content/cleaned/CH_T-03_Aklavik_ocr_clean.txt (deflated 64%)\n",
      "  adding: content/cleaned/CH_T-02_Aklavik_ocr_clean.txt (deflated 66%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /content/data-clean.zip /content/cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CKZtZLu0v3-z",
   "metadata": {
    "id": "CKZtZLu0v3-z"
   },
   "source": [
    "Add raw ocr files (uncleaned) to data-clean.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80Ti0KpDlXn6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "80Ti0KpDlXn6",
    "outputId": "d9946b53-1f2e-48ca-d9f7-4dc791a03fea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/data/ (stored 0%)\n",
      "  adding: content/data/CH_T-03_Aklavik_ocr.txt (deflated 66%)\n",
      "  adding: content/data/CH_T-02_Aklavik_ocr.txt (deflated 68%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /content/data-clean.zip /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0FlWTguAv-sM",
   "metadata": {
    "id": "0FlWTguAv-sM"
   },
   "source": [
    "Add ocr-notes to data-clean.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U63jah26ljhM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U63jah26ljhM",
    "outputId": "9a3e4f42-1814-446f-9941-9b642e6806f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/notes/ (stored 0%)\n",
      "  adding: content/notes/CH_T-03_Aklavik_notes.txt (deflated 86%)\n",
      "  adding: content/notes/CH_T-02_Aklavik_notes.txt (deflated 86%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /content/data-clean.zip /content/notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dUUqqGRMcjVH",
   "metadata": {
    "id": "dUUqqGRMcjVH"
   },
   "source": [
    "###  Download the Zip file containing all processed files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_ivwPeH6XeS5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "_ivwPeH6XeS5",
    "outputId": "ac2c683f-8dc1-4b7b-fffd-a22bb69ce491"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_01f043fa-47d4-46ee-8411-f483f423306f\", \"data-clean.zip\", 52527)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_d74862d8-6bec-4ef2-aaac-0c860c8de4a8\", \"data-ocr-scanned.zip\", 394116)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"data-clean.zip\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
